{
  "bucket": "mlcommons-inference-wg-public",
  "domain": "inference.mlcommons-storage.org",
  "title": "MLPerf Inference Benchmark Data Download",
  "index_expandable": "true",
  "datasets": {
    "DeepSeek-R1 Benchmark": [
      {
        "name": "deepseek-r1-0528",
        "subpath": "deepseek_r1/models/DeepSeek-R1-0528/",
        "title": "DeepSeek-R1-0528 model",
        "description": "DeepSeek-R1-0528 model for the DeepSeek-R1 benchmark",
        "size": "689GB"
      },
      {
        "name": "deepseek-r1-datasets-fp8-eval",
        "subpath": "deepseek_r1/datasets/",
        "title": "DeepSeek-R1 datasets",
        "description": "Full preprocessed dataset and calibration dataset for the DeepSeek-R1 benchmark",
        "destination": "./",
        "size": "163MB"
      }
    ],
    "DLRM v2 Benchmark": [
      {
        "name": "dlrm-v2-preprocessed-dataset",
        "subpath": "dlrm_preprocessed/",
        "title": "Preprocessed DLRM v2 benchmark dataset",
        "description": "Multihot Criteo Click Logs dataset preprocessed for the DLRM v2 benchmark",
        "size": "163GB"
      },
      {
        "name": "dlrm-v2-model-weights",
        "subpath": "model_weights/",
        "title": "DLRM v2 model weights",
        "description": "PyTorch model weights for the DLRM v2 benchmark",
        "size": "105GB"
      }
    ],
    "GPT-J Benchmark": [
      {
        "name": "gpt-j-model-checkpoint",
        "subpath": "gpt-j/checkpoint-final/",
        "title": "GPT-J model checkpoint",
        "description": "Model checkpoint for the GPT-J benchmark",
        "destination": "model",
        "size": "25GB"
      }
    ],
    "Llama 2 70b Benchmark": [
      {
        "name": "llama-2-70b-open-orca-dataset",
        "subpath": "open_orca/",
        "title": "Preprocessed Open Orca dataset",
        "description": "Open Orca dataset preprocessed for the Llama 2 70b benchmark",
        "size": "418MB"
      }
    ],
    "Llama 3.1 405b Benchmark": [
      {
        "name": "llama3-1-405b-calibration-dataset-512",
        "subpath": "llama3.1_405b/mlperf_llama3.1_405b_calibration_dataset_512_processed_fp16_eval.pkl/",
        "title": "Llama 3.1 405b calibration dataset",
        "description": "Calibration dataset for the Llama 3.1 405b benchmark",
        "size": "57MB"
      },
      {
        "name": "llama3-1-405b-dataset-8313",
        "subpath": "llama3.1_405b/mlperf_llama3.1_405b_dataset_8313_processed_fp16_eval.pkl/",
        "title": "Llama 3.1 405b dataset",
        "description": "Dataset for the Llama 3.1 405b benchmark",
        "size": "892MB"
      }
    ],
    "Llama 3.1 8b Benchmark": [
      {
        "name": "llama3-1-8b-cnn-eval",
        "subpath": "llama3.1_8b/datasets/cnn_eval.json",
        "title": "Full CNN evaluation dataset (Inference Datacenter)",
        "description": "CNN dataset for the Llama 3.1 8b Inference Datacenter benchmark",
        "size": "267MB"
      },
      {
        "name": "llama3-1-8b-sample-cnn-eval-5000",
        "subpath": "llama3.1_8b/datasets/sample_cnn_eval_5000.json",
        "title": "5000 samples CNN evaluation dataset (Inference Edge)",
        "description": "Sample CNN dataset for the Llama 3.1 8b Inference Edge benchmark",
        "size": "101MB"
      },
      {
        "name": "llama3-1-8b-cnn-dailymail-calibration",
        "subpath": "llama3.1_8b/datasets/cnn_dailymail_calibration.json",
        "title": "CNN-DailyMail calibration dataset",
        "description": "CNN-DailyMail calibration dataset for the Llama 3.1 8b benchmark",
        "size": "21MB"
      }
    ],
    "Mixtral 8x7b Benchmark": [
      {
        "name": "mixtral-8x7b-model-checkpoint",
        "subpath": "mixtral_8x7b/mixtral-8x7b-instruct-v0.1/",
        "title": "Mixtral 8x7b model checkpoint",
        "description": "Mixtral 8x7b model checkpoint for the Mixtral 8x7b benchmark",
        "size": "187GB"
      },
      {
        "name": "mixtral-8x7b-validation-dataset",
        "subpath": "mixtral_8x7b/09292024_mixtral_15k_mintoken2_v1.pkl",
        "title": "Mixtral 8x7b validation dataset",
        "description": "Validation dataset for the Mixtral 8x7b benchmark",
        "size": "75MB"
      },
      {
        "name": "mixtral-8x7b-calibration-dataset",
        "subpath": "mixtral_8x7b/2024.06.06_mixtral_15k_calibration_v4.pkl",
        "title": "Mixtral 8x7b calibration dataset",
        "description": "Calibration dataset for the Mixtral 8x7b benchmark",
        "size": "5.0MB"
      }
    ],
    "RGAT Benchmark": [
      {
        "name": "rgat-model",
        "subpath": "R-GAT/",
        "title": "RGAT model",
        "description": "RGAT model for the RGAT benchmark",
        "size": "53MB"
      }
    ],
    "Stable Diffusion Benchmark": [
      {
        "name": "stable-diffusion-xl-1-0-fp32-checkpoint",
        "subpath": "stable_diffusion_fp32/",
        "title": "Stable Diffusion FP32 model",
        "description": "Stable Diffusion XL 1.0 FP32 model checkpoint for the Stable Diffusion benchmark",
        "size": "14GB"
      },
      {
        "name": "stable-diffusion-xl-1-0-fp16-checkpoint",
        "subpath": "stable_diffusion_fp16/",
        "title": "Stable Diffusion FP16 model",
        "description": "Stable Diffusion XL 1.0 FP16 model checkpoint for the Stable Diffusion benchmark",
        "size": "7.0GB"
      }
    ],
    "Whisper Benchmark": [
      {
        "name": "whisper-model",
        "subpath": "Whisper/model/",
        "title": "Whisper model",
        "description": "Whisper large-v3 model for the Whisper benchmark",
        "destination": "whisper/model",
        "size": "25GB"
      },
      {
        "name": "whisper-dataset",
        "subpath": "Whisper/dataset/",
        "title": "Whisper dataset",
        "description": "LibriSpeech dataset for the Whisper benchmark",
        "destination": "whisper/dataset",
        "size": "4.6GB"
      }
    ],
    "YOLO Benchmark": [
      {
        "name": "YOLO-COCO2017-dataset",
        "subpath": "coco_safe/",
        "title": "YOLO COCO2017 dataset",
        "description": "COCO2017 filtered dataset for YOLO",
        "size": "262MB"
      }
    ]
  }
}
